Your task is to first understand the user query and devise a plan for generating a structured specification of the user's intended scientific workflow.
Here are some examples of the user query along with its corresponding plan for generating the structured specification of the intended scientific workflow:

##1
-User Query: 
{This workflow will extract the plain text content of PDF files supplied to the input port. You can encode the text as Base64 using the handy local service ("Encode Byte Array to Base 64") included with Taverna, although this requires a service that knows to decode the Base 64 back to text, which is not common. The PDF to text service makes use of the "pdftotext" executable from Xpdf.
This is a workflow component, designed to be used as a nested workflow inside a larger text mining or text processing workflow.}
Let's think step by step.
-Plan:
1. Input the PDF files.
2. Use the "Encode Byte Array to Base 64" local service to encode the text as Base64.
3. Use the "PDF to text" service to extract the plain text content of the PDF files.
4. Output the TEXT files.

##2
-User Query: 
{This workflow retrieves a fasta sequence or blast report from an id. Firstly, given some inputs (e.g. ids:EDL10223.1, format:fasta, style:default and db:emblcds), it returns the sequence in fasta format associated to those inputs using the EBI's WSDbfetch web service (see http://www.ebi.ac.uk/Tools/webservices/services/dbfetch).
Next, we obtain a Blast report from this fasta sequence and some others parameters. The final result in that case corresponds with the same fasta sequence introduced as input parameter to the "run" processor. However, it could retrieve the blast report in XML format by changing BeanShell "process_ResultTypes"   condition from the value "sequence" to "xml".}
Let's think step by step.
-Plan：
1. Input the parameters: ids, db, style, format, title, email.
2. Use the EBI's WSDbfetch web service to retrieve the fasta sequence associated with the input parameters.
3. Run a Blast report using the retrieved fasta sequence and other parameters.
4. If the desired output is the fasta sequence, return the same fasta sequence introduced as input parameter to the "run" processor.
5. If the desired output is the Blast report in XML format, change the BeanShell "process_ResultTypes" condition from the value "sequence" to "xml" and retrieve the Blast report in XML format.
6. Output the desired result to the user.

##3
-User Query: 
{1. split the list of identifiers using allows separators,
2. build a MIQL query with these identifier in preparation of running a PSICQUIC request,
3. Retrieve molecular interaction from IntAct's PSICQUIC service using the generated MIQL statement,
4. filter the list of returned MITAB using the list of identifiers and only keep those lines where both participating molecules are members of the input list,
5. return the count and list of filtered MITAB lines.}
Let's think step by step.
-Plan：
1. Input the identifiers.
2. Split the identifiers using the allowed separators.
3. Build a MIQL query statement using the split identifiers.
4. Retrieve molecular interactions from IntAct's PSICQUIC service using the MIQL query statement.
5. Filter the returned MITAB using the list of identifiers.
6. Output the count and list of filtered MITAB lines.

##4
-User Query: 
{This workflow finds disease relevant to the query string via the following steps: 1. A user query: a list of terms or boolean query - look at the Apache Lucene project for all details. E.g.: (EZH2 OR "Enhancer of Zeste" +(mutation chromatin) -clinical); consider adding 'ProteinSynonymsToQuery' in front of the input if your query is a protein. 2. Retrieve documents: finds 'maximumNumberOfHits' relevant documents (abstract+title) based on query (the AIDA service inside is based on Apache's Lucene) 3. Discover proteins: extract proteins discovered in the set of relevant abstracts with a 'named entity recognizer' trained on genomic terms using a Bayesian approach; the AIDA service inside is based on LingPipe. This subworkflow also 'filters' false positives from the discovered protein by requiring a discovery has a valid UniProt ID. Martijn Schuemie's service to do that contains only human UniProt IDs, which is why this workflow only works for human proteins. 4. Link proteins to disease contained in the OMIM disease database (with a service from Japan that interrogates OMIM) Workflow by Marco Roos (AID = Adaptive Information Disclosure, University of Amsterdam; http://adaptivedisclosure.org) Text mining services by Sophia Katrenko and Edgar Meij (AID), and Martijn Schuemie (BioSemantics, Erasmus University Rotterdam). OMIM service from the Center for Information Biology and DNA Data Bank of Japan, National Institute of Genetics, director Hideaki Sugawara (see http://xml.nig.ac.jp) Changes to our original BioAID_DiseaseDiscovery workflow: * Use of Martijn Schuemie's synsets service to * provide uniprot ids to discovered proteins * filter false positive discoveries, only proteins with a uniprot id go through; this introduces some false negatives (e.g. discovered proteins with a name shorter than 3 characters) * solve a major issue with the original workflow where some false positives could contribute disproportionately to the number of discovered diseases * Counting of results in various ways.}
Let's think step by step.
-Plan：
1. Input the query string.
2. Retrieve relevant documents including abstracts and titles.
3. Extract proteins from relevant documents' abstracts.
4. Link proteins to diseases.
5. Output the relevant documents, discovered proteins, and discovered diseases.

##5
-User Query: 
{Given a structure or structure entry identifer (e.g. PDB:1crn), return the structure in PDB format.
If a structure identifier, in database:identifier format, is input the EBI's WSDbfetch web service (see http://www.ebi.ac.uk/Tools/webservices/services/dbfetch) is used to retrive the structure in PDB format. Otherwise the input is assumed to be a formated structure and is passed through to the output.}
Let's think step by step.
-Plan:
1. Input the structure or structure entry identifier.
2. Check if the input is in the format database:identifier.
3. If the input is in the format database:identifier, use the EBI's WSDbfetch web service to retrieve the structure in PDB format.
4. If the input is not in the format database:identifier, assume it is a formatted structure and pass it through to the output.
5. Output the structure in PDB format.

##6
-User Query: 
{This workflow performs an NCBI blast at the EBI. It accepts a protein sequence as input. Default values have been set for the search database (Uniprot), the number of hits to return (10), and all scoring and matrix options. These can be changed in the workflow by altering the string constant values if required. This workflow uses the new EBI services. They are asynchronous and so require looping over the nested workflow (Status) until the workflow has finished. Many of the EBI services now work in this way, so you can use this workflow as an example of the invocation pattern and looping configuration.}
Let's think step by step.
-Plan:
1. Input the protein sequence and user's email.
2. Perform an NCBI blast at the EBI using the input protein sequence.
3. Set default values for the search database (Uniprot), the number of hits to return (10), and all scoring and matrix options. These can be changed if required.
4. Retrieve the sequences for the top 10 hits from the UniProt database.
5. Analyse the retrieved sequences using InterproScan to determine functional domains and motifs in each sequence.
6. Display the results in a graphical format.
7. Loop over the nested workflows (Status) until the workflow has finished.

##7
-User Query:
{Given an identifier for genome sequence (by default, genome of Mycoplasma genitalium: refseq:NC_000908) or raw sequence data in FASTA format, this workflow calculates and graphs the following properties using the G-language Genome Analysis Environment: GC skew (gcskew), cumulative GC skew (gcskew_cumulative), GC skew of coding/intergenic/GC3 (genomicskew), GC content with sliding windows (gcwin), replication origin and terminus (find_ori_ter), codon usage table (codon_usage), the Codon Adaptation Index (cai), nucleotide composition around the start/stop codons calculated with different information theory measures (view_cds, base_information_content, base_entropy, base_relative_entropy). See http://www.g-language.org/ for more information about the G-language Genome Analysis Environment.}
Let's think step by step.
-Plan:
1. Input the identifier for genome sequence or raw sequence data in FASTA format.
2. Use the G-language Genome Analysis Environment to calculate GC skew, cumulative GC skew, GC skew of coding/intergenic/GC3, GC content with sliding windows, replication origin and terminus, codon usage table, the Codon Adaptation Index, nucleotide composition around the start/stop codons calculated with different information theory measures.
3. Graph the calculated properties.

##8
-User Query: 
{Perform a ClustalW2 alignment of protein sequences using the EMBL-EBIâ€™s ClustalW2 (SOAP) service (see http://www.ebi.ac.uk/Tools/webservices/services/msa/clustalw2_soap). This workflow uses the new EBI services, which are asynchronous and require looping over the nested workflow (Status) until the workflow has finished. Many of the EBI services now work in this way, so you can use this workflow as an example of the invocation pattern and looping configuration.}
Let's think step by step.
-Plan:
1. Input the protein sequences.
2. Use the EMBL-EBI's ClustalW2 (SOAP) service to perform a ClustalW2 alignment of the protein sequences.
3. Loop over the nested workflow (Status) until the alignment process is finished.
4. Output the aligned protein sequences.


-User Query:
{!!!}
Let's think step by step.
-Plan:



